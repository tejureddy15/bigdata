1)

Sol:

sqoop import-all-tables \
--connect jdbc:mysql://localhost/retail_db \
--username root \
--password cloudera \
--as-avrodatafile \
--compress \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec \
--warehouse-dir /user/cloudera/Udemy/retail_db.db \
-m 1

[cloudera@quickstart ~]$ hadoop fs -ls /user/cloudera/Udemy/retail_db.db
Found 15 items
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 13:59 /user/cloudera/Udemy/retail_db.db/categories
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:00 /user/cloudera/Udemy/retail_db.db/customer_new
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:00 /user/cloudera/Udemy/retail_db.db/customers
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:01 /user/cloudera/Udemy/retail_db.db/departments
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:01 /user/cloudera/Udemy/retail_db.db/departments_export
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:02 /user/cloudera/Udemy/retail_db.db/departments_new
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:02 /user/cloudera/Udemy/retail_db.db/order_items
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:03 /user/cloudera/Udemy/retail_db.db/orders
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:04 /user/cloudera/Udemy/retail_db.db/product_hive
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:04 /user/cloudera/Udemy/retail_db.db/products
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:05 /user/cloudera/Udemy/retail_db.db/products_external
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:05 /user/cloudera/Udemy/retail_db.db/products_replica
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:06 /user/cloudera/Udemy/retail_db.db/result
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:06 /user/cloudera/Udemy/retail_db.db/sqoop_inc
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 14:07 /user/cloudera/Udemy/retail_db.db/sqoop_inc1

[cloudera@quickstart ~]$ hadoop fs -ls /user/cloudera/Udemy/retail_db.db/orders
Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2018-11-01 14:03 /user/cloudera/Udemy/retail_db.db/orders/_SUCCESS
-rw-r--r--   1 cloudera cloudera     660171 2018-11-01 14:03 /user/cloudera/Udemy/retail_db.db/orders/part-m-00000.avro

scala> import com.databricks.spark.avro._
import com.databricks.spark.avro._

scala> val ordersDF = sqlContext.read.avro("/user/cloudera/Udemy/retail_db.db/orders")
ordersDF: org.apache.spark.sql.DataFrame = [order_id: int, order_date: bigint, order_customer_id: int, order_status: string]

scala> ordersDF.count()
res0: Long = 68883

[cloudera@quickstart ~]$ hadoop fs -get /user/cloudera/Udemy/retail_db.db/orders/part-m-00000.avro /home/cloudera/Documents/Udemy/OrdersAvro

[cloudera@quickstart ~]$ avro-tools getschema /home/cloudera/Documents/Udemy/OrdersAvro/part-m-00000.avro > orders.avsc
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.

[cloudera@quickstart ~]$ hadoop fs -mkdir /user/hive/schemas

[cloudera@quickstart ~]$ hadoop fs -put orders.avsc /user/hive/schemas

[cloudera@quickstart ~]$ hadoop fs -ls /user/hive/schemas
Found 1 items
-rw-r--r--   1 cloudera supergroup        708 2018-11-01 14:30 /user/hive/schemas/orders.avsc

hive> create table orders_sqoop stored as avro location '/user/cloudera/Udemy/retail_db.db/orders' tblproperties ('avro.schema.url' = '/user/hive/schemas/orders.avsc'); 
OK
Time taken: 0.695 seconds

hive> desc orders_sqoop
    > ;
OK
order_id            	int                 	                    
order_date          	bigint              	                    
order_customer_id   	int                 	                    
order_status        	string              	                    
Time taken: 0.228 seconds, Fetched: 4 row(s)

======================================================================================================================================================================

2)

sqoop import \
--connect jdbc:mysql://localhost/retail_db \
--username root \
--password cloudera \
--table customers \
--as-textfile \
--fields-terminated-by '\t' \
--target-dir /user/cloudera/problem3/customers/textfile

18/11/01 14:57:23 INFO mapreduce.ImportJobBase: Retrieved 12435 records.

[cloudera@quickstart ~]$ hadoop fs -ls /user/cloudera/problem3/customers/textfile
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2018-11-01 14:57 /user/cloudera/problem3/customers/textfile/_SUCCESS
-rw-r--r--   1 cloudera cloudera     237145 2018-11-01 14:57 /user/cloudera/problem3/customers/textfile/part-m-00000
-rw-r--r--   1 cloudera cloudera     237965 2018-11-01 14:57 /user/cloudera/problem3/customers/textfile/part-m-00001
-rw-r--r--   1 cloudera cloudera     238092 2018-11-01 14:57 /user/cloudera/problem3/customers/textfile/part-m-00002
-rw-r--r--   1 cloudera cloudera     240323 2018-11-01 14:57 /user/cloudera/problem3/customers/textfile/part-m-00003

[cloudera@quickstart ~]$ hadoop fs -cat /user/cloudera/problem3/customers/textfile/* | wc -l
12435

[cloudera@quickstart ~]$ hadoop fs -cat /user/cloudera/problem3/customers/textfile/part-m-00000 | head

1       Richard	Hernandez	XXXXXXXXX	XXXXXXXXX	6303 Heather Plaza	Brownsville	TX	78521
2	Mary	Barrett	XXXXXXXXX	XXXXXXXXX	9526 Noble Embers Ridge	Littleton	CO	80126
3	Ann	Smith   XXXXXXXXX	XXXXXXXXX	3422 Blue Pioneer Bend	Caguas	PR	00725
4	Mary	Jones	XXXXXXXXX	XXXXXXXXX	8324 Little Common	San Marcos	CA	92069
5	Robert	Hudson	XXXXXXXXX	XXXXXXXXX	10 Crystal River Mall 	Caguas	PR	00725
6	Mary	Smith	XXXXXXXXX	XXXXXXXXX	3151 Sleepy Quail Promenade	Passaic	NJ	07055
7	Melissa	Wilcox	XXXXXXXXX	XXXXXXXXX	9453 High Concession	Caguas	PR	00725
8	Megan	Smith	XXXXXXXXX	XXXXXXXXX	3047 Foggy Forest Plaza	Lawrence	MA	01841
9	Mary	Perez	XXXXXXXXX	XXXXXXXXX	3616 Quaking Street	Caguas	PR	00725
10	Melissa	Smith	XXXXXXXXX	XXXXXXXXX	8598 Harvest Beacon Plaza	Stafford	VA	22554
cat: Unable to write to output stream.

scala> case class Customers(
     | customer_id : Int,
     | customer_fname : String,
     | customer_lname : String,
     | customer_email : String
     | )
defined class Customers

scala> val custDF = sc.textFile("/user/cloudera/problem3/customers/textfile").
     | map(rec => rec.split('\t')).
     | map(rec => Customers(rec(0).toInt,rec(1),rec(2),rec(3))).
     | toDF()
custDF: org.apache.spark.sql.DataFrame = [customer_id: int, customer_fname: string, customer_lname: string, customer_email: string]

scala> custDF.show(5)
+-----------+--------------+--------------+--------------+
|customer_id|customer_fname|customer_lname|customer_email|
+-----------+--------------+--------------+--------------+
|          1|       Richard|     Hernandez|     XXXXXXXXX|
|          2|          Mary|       Barrett|     XXXXXXXXX|
|          3|           Ann|         Smith|     XXXXXXXXX|
|          4|          Mary|         Jones|     XXXXXXXXX|
|          5|        Robert|        Hudson|     XXXXXXXXX|
+-----------+--------------+--------------+--------------+
only showing top 5 rows


scala> val custDF1 = custDF.map(rec => rec(0) + "|" + rec(1) + "|" + rec(2) + "|" + rec(3)).saveAsTextFile("/user/cloudera/problem3/customers/textfile/output")
custDF1: Unit = ()

[cloudera@quickstart ~]$ hadoop fs -cat /user/cloudera/problem3/customers/textfile/output/part-00000 | head
1|Richard|Hernandez|XXXXXXXXX
2|Mary|Barrett|XXXXXXXXX
3|Ann|Smith|XXXXXXXXX
4|Mary|Jones|XXXXXXXXX
5|Robert|Hudson|XXXXXXXXX
6|Mary|Smith|XXXXXXXXX
7|Melissa|Wilcox|XXXXXXXXX
8|Megan|Smith|XXXXXXXXX
9|Mary|Perez|XXXXXXXXX
10|Melissa|Smith|XXXXXXXXX
cat: Unable to write to output stream.

[cloudera@quickstart ~]$ hadoop fs -cat /user/cloudera/problem3/customers/textfile/output/* | wc -l
12435
 
======================================================================================================================================================================

3)

sqoop import \
--connect jdbc:mysql://localhost/retail_db \
--username root \
--password cloudera \
--table customers \
--target-dir /user/cloudera/Udemy/problem3/customers/permissions

18/11/01 16:05:21 INFO mapreduce.ImportJobBase: Retrieved 12435 records.

mysql> select count(*) from customers;
+----------+
| count(*) |
+----------+
|    12435 |
+----------+
1 row in set (0.02 sec)

[cloudera@quickstart ~]$ hadoop fs -ls /user/cloudera/Udemy/problem3/customers/permissions
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2018-11-01 16:05 /user/cloudera/Udemy/problem3/customers/permissions/_SUCCESS
-rw-r--r--   1 cloudera cloudera     237145 2018-11-01 16:05 /user/cloudera/Udemy/problem3/customers/permissions/part-m-00000
-rw-r--r--   1 cloudera cloudera     237965 2018-11-01 16:05 /user/cloudera/Udemy/problem3/customers/permissions/part-m-00001
-rw-r--r--   1 cloudera cloudera     238092 2018-11-01 16:05 /user/cloudera/Udemy/problem3/customers/permissions/part-m-00002
-rw-r--r--   1 cloudera cloudera     240323 2018-11-01 16:05 /user/cloudera/Udemy/problem3/customers/permissions/part-m-00003

[cloudera@quickstart ~]$ hadoop fs -chmod 765 /user/cloudera/Udemy/problem3/customers/permissions/*

[cloudera@quickstart ~]$ hadoop fs -ls /user/cloudera/Udemy/problem3/customers/permissions
Found 5 items
-rwxrw-r-x   1 cloudera cloudera          0 2018-11-01 16:05 /user/cloudera/Udemy/problem3/customers/permissions/_SUCCESS
-rwxrw-r-x   1 cloudera cloudera     237145 2018-11-01 16:05 /user/cloudera/Udemy/problem3/customers/permissions/part-m-00000
-rwxrw-r-x   1 cloudera cloudera     237965 2018-11-01 16:05 /user/cloudera/Udemy/problem3/customers/permissions/part-m-00001
-rwxrw-r-x   1 cloudera cloudera     238092 2018-11-01 16:05 /user/cloudera/Udemy/problem3/customers/permissions/part-m-00002
-rwxrw-r-x   1 cloudera cloudera     240323 2018-11-01 16:05 /user/cloudera/Udemy/problem3/customers/permissions/part-m-00003

======================================================================================================================================================================

4)

sqoop import \
--connect jdbc:mysql://localhost/retail_db \
--username root \
--password cloudera \
--table customers \
--target-dir /user/cloudera/Udemy/customers/delate \
--fields-terminated-by '^' \
--compress \
--compression-codec org.apache.hadoop.io.compress.DeflateCodec \
--columns "customer_id,customer_fname,customer_lname,customer_street"

18/11/01 17:00:39 INFO mapreduce.ImportJobBase: Retrieved 12435 records.

[cloudera@quickstart ~]$ hadoop fs -ls /user/cloudera/Udemy/customers/delate
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2018-11-01 17:00 /user/cloudera/Udemy/customers/delate/_SUCCESS
-rw-r--r--   1 cloudera cloudera      44163 2018-11-01 17:00 /user/cloudera/Udemy/customers/delate/part-m-00000.deflate
-rw-r--r--   1 cloudera cloudera      44205 2018-11-01 17:00 /user/cloudera/Udemy/customers/delate/part-m-00001.deflate
-rw-r--r--   1 cloudera cloudera      44269 2018-11-01 17:00 /user/cloudera/Udemy/customers/delate/part-m-00002.deflate
-rw-r--r--   1 cloudera cloudera      44396 2018-11-01 17:00 /user/cloudera/Udemy/customers/delate/part-m-00003.deflate


======================================================================================================================================================================

5)

sqoop import \
--connect jdbc:mysql://localhost/retail_db \
--username root \
--password cloudera \
--table orders \
--as-parquetfile \
--target-dir /user/cloudera/Udemy/orders/parquet

18/11/01 17:18:21 INFO mapreduce.ImportJobBase: Retrieved 68883 records.

[cloudera@quickstart ~]$ hadoop fs -ls /user/cloudera/Udemy/orders/parquet
Found 6 items
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 17:16 /user/cloudera/Udemy/orders/parquet/.metadata
drwxr-xr-x   - cloudera cloudera          0 2018-11-01 17:18 /user/cloudera/Udemy/orders/parquet/.signals
-rw-r--r--   1 cloudera cloudera     147312 2018-11-01 17:18 /user/cloudera/Udemy/orders/parquet/20da0b60-19fb-4996-884d-be4844413724.parquet
-rw-r--r--   1 cloudera cloudera     147465 2018-11-01 17:18 /user/cloudera/Udemy/orders/parquet/2cdb328c-f32b-412b-9a68-71b0dc9ebdff.parquet
-rw-r--r--   1 cloudera cloudera     151742 2018-11-01 17:18 /user/cloudera/Udemy/orders/parquet/4baa1091-729b-4f05-a7d5-43bfcbe2c8ef.parquet
-rw-r--r--   1 cloudera cloudera     147265 2018-11-01 17:18 /user/cloudera/Udemy/orders/parquet/a9cd1e10-195c-41c5-95cc-0007f99e28bc.parquet

scala> val ordersDF = sqlContext.read.parquet("/user/cloudera/Udemy/orders/parquet")
ordersDF: org.apache.spark.sql.DataFrame = [order_id: int, order_date: bigint, order_customer_id: int, order_status: string]

scala> ordersDF.count()
res3: Long = 68883                                                              

scala> sqlContext.setConf("spark.sql.avro.compression.codec","snappy")

scala> ordersDF.write.avro("/user/cloudera/Udemy/orders/parquet/output")

[cloudera@quickstart ~]$ hadoop fs -ls /user/cloudera/Udemy/orders/parquet/output
Found 5 items
-rw-r--r--   1 cloudera cloudera          0 2018-11-01 17:23 /user/cloudera/Udemy/orders/parquet/output/_SUCCESS
-rw-r--r--   1 cloudera cloudera     163875 2018-11-01 17:23 /user/cloudera/Udemy/orders/parquet/output/part-r-00000-d08df272-d6f1-469c-8c58-fc417332837b.avro
-rw-r--r--   1 cloudera cloudera     164048 2018-11-01 17:23 /user/cloudera/Udemy/orders/parquet/output/part-r-00001-d08df272-d6f1-469c-8c58-fc417332837b.avro
-rw-r--r--   1 cloudera cloudera     169178 2018-11-01 17:23 /user/cloudera/Udemy/orders/parquet/output/part-r-00002-d08df272-d6f1-469c-8c58-fc417332837b.avro
-rw-r--r--   1 cloudera cloudera     163894 2018-11-01 17:23 /user/cloudera/Udemy/orders/parquet/output/part-r-00003-d08df272-d6f1-469c-8c58-fc417332837b.avro

======================================================================================================================================================================

6)

sqoop import \
--connect jdbc:mysql://localhost/retail_db \
--username root \
--password cloudera \
--table order_items \
--target-dir /user/cloudera/Udemy/problem3/order-items \
--as-textfile

18/11/01 17:40:41 INFO mapreduce.ImportJobBase: Retrieved 172198 records.

[cloudera@quickstart ~]$ hadoop fs -cat /user/cloudera/Udemy/problem3/order-items/* | wc -l
172198


sqoop import \
--connect jdbc:mysql://localhost/retail_db \
--username root \
--password cloudera \
--table products \
--as-textfile \
--target-dir /user/cloudera/Udemy/problem3/products \
--columns "product_id,product_name"

18/11/01 17:44:08 INFO mapreduce.ImportJobBase: Retrieved 1345 records.

[cloudera@quickstart ~]$ hadoop fs -cat /user/cloudera/Udemy/problem3/products/* | wc -l
1345

[cloudera@quickstart ~]$ hadoop fs -cat /user/cloudera/Udemy/problem3/order-items/part-m-00000 | head
1,1,957,1,299.98,299.98
2,2,1073,1,199.99,199.99
3,2,502,5,250.0,50.0
4,2,403,1,129.99,129.99
5,4,897,2,49.98,24.99
6,4,365,5,299.95,59.99
7,4,502,3,150.0,50.0
8,4,1014,4,199.92,49.98
9,5,957,1,299.98,299.98
10,5,365,5,299.95,59.99
cat: Unable to write to output stream.

[cloudera@quickstart ~]$ hadoop fs -cat /user/cloudera/Udemy/problem3/products/part-m-00000 | head
1,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U
2,Under Armour Men's Highlight MC Football Clea
3,Under Armour Men's Renegade D Mid Football Cl
4,Under Armour Men's Renegade D Mid Football Cl
5,Riddell Youth Revolution Speed Custom Footbal
6,Jordan Men's VI Retro TD Football Cleat
7,Schutt Youth Recruit Hybrid Custom Football H
8,Nike Men's Vapor Carbon Elite TD Football Cle
9,Nike Adult Vapor Jet 3.0 Receiver Gloves
10,Under Armour Men's Highlight MC Football Clea
cat: Unable to write to output stream.

case class OrderItems(
order_item_id : Int,
order_item_order_id : Int,
order_item_product_id : Int,
order_item_quantity : Int,
order_item_subtotal : Float,
order_item_product_price : Float
)

scala> val ordersDF = sc.textFile("/user/cloudera/Udemy/problem3/order-items").
     | map(rec => rec.split(',')).
     | map(rec => OrderItems(rec(0).toInt,rec(1).toInt,rec(2).toInt,rec(3).toInt,rec(4).toFloat,rec(5).toFloat)).
     | toDF()
ordersDF: org.apache.spark.sql.DataFrame = [order_item_id: int, order_item_order_id: int, order_item_product_id: int, order_item_quantity: int, order_item_subtotal: float, order_item_product_price: float]

scala> ordersDF.show()
+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+
|order_item_id|order_item_order_id|order_item_product_id|order_item_quantity|order_item_subtotal|order_item_product_price|
+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+
|            1|                  1|                  957|                  1|             299.98|                  299.98|
|            2|                  2|                 1073|                  1|             199.99|                  199.99|
|            3|                  2|                  502|                  5|              250.0|                    50.0|
|            4|                  2|                  403|                  1|             129.99|                  129.99|
|            5|                  4|                  897|                  2|              49.98|                   24.99|
|            6|                  4|                  365|                  5|             299.95|                   59.99|
|            7|                  4|                  502|                  3|              150.0|                    50.0|
|            8|                  4|                 1014|                  4|             199.92|                   49.98|
|            9|                  5|                  957|                  1|             299.98|                  299.98|
|           10|                  5|                  365|                  5|             299.95|                   59.99|
|           11|                  5|                 1014|                  2|              99.96|                   49.98|
|           12|                  5|                  957|                  1|             299.98|                  299.98|
|           13|                  5|                  403|                  1|             129.99|                  129.99|
|           14|                  7|                 1073|                  1|             199.99|                  199.99|
|           15|                  7|                  957|                  1|             299.98|                  299.98|
|           16|                  7|                  926|                  5|              79.95|                   15.99|
|           17|                  8|                  365|                  3|             179.97|                   59.99|
|           18|                  8|                  365|                  5|             299.95|                   59.99|
|           19|                  8|                 1014|                  4|             199.92|                   49.98|
|           20|                  8|                  502|                  1|               50.0|                    50.0|
+-------------+-------------------+---------------------+-------------------+-------------------+------------------------+
only showing top 20 rows

scala> val grouped = ordersDF.groupBy("order_item_product_id").agg(sum("order_item_subtotal").alias("total_revenue"))
grouped: org.apache.spark.sql.DataFrame = [order_item_product_id: int, total_revenue: double]

scala> grouped.show(10)
+---------------------+------------------+                                      
|order_item_product_id|     total_revenue|
+---------------------+------------------+
|                  835|29686.719787597656|
|                  235|29601.540645599365|
|                   35|10399.350357055664|
|                   37|27327.190544128418|
|                  642|           27840.0|
|                   44| 56330.61164474487|
|                  845|  10799.6396484375|
|                  646| 20697.92999267578|
|                  647| 9449.300384521484|
|                  249| 46559.59103393555|
+---------------------+------------------+
only showing top 10 rows


scala> val grouped = ordersDF.groupBy("order_item_product_id").agg(round(sum("order_item_subtotal"),2).alias("total_revenue"))
grouped: org.apache.spark.sql.DataFrame = [order_item_product_id: int, total_revenue: double]

scala> grouped.show(10)
+---------------------+-------------+                                           
|order_item_product_id|total_revenue|
+---------------------+-------------+
|                  835|     29686.72|
|                  235|     29601.54|
|                   35|     10399.35|
|                   37|     27327.19|
|                  642|      27840.0|
|                   44|     56330.61|
|                  845|     10799.64|
|                  646|     20697.93|
|                  647|       9449.3|
|                  249|     46559.59|
+---------------------+-------------+
only showing top 10 rows


scala> val ordered = grouped.orderBy(col("total_revenue").desc)
ordered: org.apache.spark.sql.DataFrame = [order_item_product_id: int, total_revenue: double]

scala> ordered.show(10)
+---------------------+-------------+                                           
|order_item_product_id|total_revenue|
+---------------------+-------------+
|                 1004|   6929653.69|
|                  365|   4421143.14|
|                  957|   4118425.57|
|                  191|    3667633.2|
|                  502|    3147800.0|
|                 1073|   3099845.09|
|                  403|   2891757.66|
|                 1014|   2888993.91|
|                  627|   1269082.67|
|                  565|      67830.0|
+---------------------+-------------+
only showing top 10 rows


scala> val ordered = grouped.orderBy(col("total_revenue").desc).limit(10)
ordered: org.apache.spark.sql.DataFrame = [order_item_product_id: int, total_revenue: double]

scala> ordered.show()
+---------------------+-------------+                                           
|order_item_product_id|total_revenue|
+---------------------+-------------+
|                 1004|   6929653.69|
|                  365|   4421143.14|
|                  957|   4118425.57|
|                  191|    3667633.2|
|                  502|    3147800.0|
|                 1073|   3099845.09|
|                  403|   2891757.66|
|                 1014|   2888993.91|
|                  627|   1269082.67|
|                  565|      67830.0|
+---------------------+-------------+

scala> ordered.map(rec => rec(0) + ":" + rec(1)).saveAsTextFile("/user/cloudera/Udemy/problem3/order-items-output")
18/11/01 21:06:23 WARN hdfs.DFSClient: Caught exception 
java.lang.InterruptedException
	at java.lang.Object.wait(Native Method)
	at java.lang.Thread.join(Thread.java:1281)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.closeResponder(DFSOutputStream.java:967)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.endBlock(DFSOutputStream.java:705)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:894)


[cloudera@quickstart ~]$ hadoop fs -ls /user/cloudera/Udemy/problem3/order-items-output
Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2018-11-01 21:06 /user/cloudera/Udemy/problem3/order-items-output/_SUCCESS
-rw-r--r--   1 cloudera cloudera        148 2018-11-01 21:06 /user/cloudera/Udemy/problem3/order-items-output/part-00000
[cloudera@quickstart ~]$ hadoop fs -cat /user/cloudera/Udemy/problem3/order-items-output/part-00000
1004:6929653.69
365:4421143.14
957:4118425.57
191:3667633.2
502:3147800.0
1073:3099845.09
403:2891757.66
1014:2888993.91
627:1269082.67
565:67830.0

======================================================================================================================================================================

7)

sqoop import \
--connect jdbc:mysql://localhost/retail_db \
--username root \
--password cloudera \
--table orders \
--as-parquetfile \
--target-dir /user/cloudera/Udemy/problem3/Orders/parquet

Question wrong. 

Read Sequence file

Write as Sequence file

======================================================================================================================================================================

8)

scala> val prodTable = sqlContext.table("default.product_ranked")
prodTable: org.apache.spark.sql.DataFrame = [product_id: int, product_category_id: int, product_name: string, product_description: string, product_price: double, product_image: string]

scala> prodTable.show(5)
+----------+-------------------+--------------------+-------------------+-------------+--------------------+
|product_id|product_category_id|        product_name|product_description|product_price|       product_image|
+----------+-------------------+--------------------+-------------------+-------------+--------------------+
|         1|                  2|Quest Q64 10 FT. ...|                   |        59.98|http://images.acm...|
|         2|                  2|Under Armour Men'...|                   |       129.99|http://images.acm...|
|         3|                  2|Under Armour Men'...|                   |        89.99|http://images.acm...|
|         4|                  2|Under Armour Men'...|                   |        89.99|http://images.acm...|
|         5|                  2|Riddell Youth Rev...|                   |       199.99|http://images.acm...|
+----------+-------------------+--------------------+-------------------+-------------+--------------------+
only showing top 5 rows


scala> val prodAgg = prodTable.groupBy(col("product_category_id")).agg(max("product_price"))
prodAgg: org.apache.spark.sql.DataFrame = [product_category_id: int, max(product_price): double]

scala> val prodAgg = prodTable.groupBy(col("product_category_id")).agg(max("product_price").alias("max_price"))
prodAgg: org.apache.spark.sql.DataFrame = [product_category_id: int, max_price: double]

scala> prodAgg.show(5)
+-------------------+---------+                                                 
|product_category_id|max_price|
+-------------------+---------+
|                 31|   899.99|
|                 32|   999.99|
|                 33|   169.99|
|                 34|   169.99|
|                 35|   299.99|
+-------------------+---------+
only showing top 5 rows

scala> val prodAgg = prodTable.groupBy(col("product_category_id").alias("prod_cat_id")).agg(max("product_price").alias("max_price"))
prodAgg: org.apache.spark.sql.DataFrame = [prod_cat_id: int, max_price: double]

scala> val finale = prodAgg.join(prodTable, prodAgg("prod_cat_id") === prodTable("product_category_id") && prodAgg("max_price") === prodTable("product_price"))
finale: org.apache.spark.sql.DataFrame = [prod_cat_id: int, max_price: double, product_id: int, product_category_id: int, product_name: string, product_description: string, product_price: double, product_image: string]

scala> finale.show(5)
+-----------+---------+----------+-------------------+--------------------+-------------------+-------------+--------------------+
|prod_cat_id|max_price|product_id|product_category_id|        product_name|product_description|product_price|       product_image|
+-----------+---------+----------+-------------------+--------------------+-------------------+-------------+--------------------+
|         31|   899.99|       685|                 31|TaylorMade SLDR I...|                   |       899.99|http://images.acm...|
|         32|   999.99|       694|                 32|Callaway Women's ...|                   |       999.99|http://images.acm...|
|         32|   999.99|       695|                 32|Callaway Women's ...|                   |       999.99|http://images.acm...|
|         33|   169.99|       739|                 33|Nike TW 14 Mesh G...|                   |       169.99|http://images.acm...|
|         33|   169.99|       740|                 33|Nike TW 14 Mesh G...|                   |       169.99|http://images.acm...|
+-----------+---------+----------+-------------------+--------------------+-------------------+-------------+--------------------+
only showing top 5 rows

======================================================================================================================================================================













































